{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cde3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2070\n",
      "TensorBoard logs seront sauvegardés dans: runs/mnist_experiment\n",
      "Pour visualiser les métriques: tensorboard --logdir=runs\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1: Imports et configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Configuration de matplotlib pour de meilleurs graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration TensorBoard\n",
    "log_dir = \"runs/mnist_experiment\"\n",
    "if os.path.exists(log_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Configuration du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"TensorBoard logs seront sauvegardés dans: {log_dir}\")\n",
    "print(\"Pour visualiser les métriques: tensorboard --logdir=runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture du modèle OPTIMISÉ avec Sequential:\n",
      "EnhancedMNISTNet(\n",
      "  (conv_block1): Sequential(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout1): Dropout2d(p=0.1, inplace=False)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout2): Dropout2d(p=0.15, inplace=False)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu5): ReLU(inplace=True)\n",
      "    (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace=True)\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout3): Dropout2d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "    (bn_fc1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_fc1): ReLU(inplace=True)\n",
      "    (dropout4): Dropout(p=0.4, inplace=False)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (bn_fc2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_fc2): ReLU(inplace=True)\n",
      "    (dropout5): Dropout(p=0.3, inplace=False)\n",
      "    (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn_fc3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_fc3): ReLU(inplace=True)\n",
      "    (dropout6): Dropout(p=0.2, inplace=False)\n",
      "    (fc4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (log_softmax): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n",
      "\n",
      "Paramètres totaux: 4,168,650\n",
      "Paramètres entraînables: 4,168,650\n",
      "Graphe du modèle optimisé ajouté à TensorBoard\n",
      "Graphe du modèle optimisé ajouté à TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Définition du modèle OPTIMISÉ avec Sequential\n",
    "from collections import OrderedDict\n",
    "\n",
    "class EnhancedMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedMNISTNet, self).__init__()\n",
    "        \n",
    "        # Bloc convolutionnel 1 - Extraction de caractéristiques de bas niveau\n",
    "        self.conv_block1 = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1, 64, 3, padding=1)),#1, 28, 28 ->64, 28, 28\n",
    "            ('bn1', nn.BatchNorm2d(64)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv2', nn.Conv2d(64, 64, 3, padding=1)), #64, 28, 28\n",
    "            ('bn2', nn.BatchNorm2d(64)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2)), #64, 14, 14\n",
    "            ('dropout1', nn.Dropout2d(0.1))\n",
    "        ]))\n",
    "        \n",
    "        # Bloc convolutionnel 2 - Caractéristiques de niveau moyen\n",
    "        self.conv_block2 = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv2d(64, 128, 3, padding=1)), #128, 14, 14\n",
    "            ('bn3', nn.BatchNorm2d(128)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('conv4', nn.Conv2d(128, 128, 3, padding=1)),\n",
    "            ('bn4', nn.BatchNorm2d(128)),\n",
    "            ('relu4', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(2, 2)),\n",
    "            ('dropout2', nn.Dropout2d(0.15))\n",
    "        ]))\n",
    "        \n",
    "        # Bloc convolutionnel 3 - Caractéristiques de haut niveau\n",
    "        self.conv_block3 = nn.Sequential(OrderedDict([\n",
    "            ('conv5', nn.Conv2d(128, 256, 3, padding=1)),\n",
    "            ('bn5', nn.BatchNorm2d(256)),\n",
    "            ('relu5', nn.ReLU(inplace=True)),\n",
    "            ('conv6', nn.Conv2d(256, 256, 3, padding=1)),\n",
    "            ('bn6', nn.BatchNorm2d(256)),\n",
    "            ('relu6', nn.ReLU(inplace=True)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2)),\n",
    "            ('dropout3', nn.Dropout2d(0.2))\n",
    "        ]))\n",
    "        \n",
    "        # Couches fully connected\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(256 * 3 * 3, 1024)),\n",
    "            ('bn_fc1', nn.BatchNorm1d(1024)),\n",
    "            ('relu_fc1', nn.ReLU(inplace=True)),\n",
    "            ('dropout4', nn.Dropout(0.4)),\n",
    "            ('fc2', nn.Linear(1024, 512)),\n",
    "            ('bn_fc2', nn.BatchNorm1d(512)),\n",
    "            ('relu_fc2', nn.ReLU(inplace=True)),\n",
    "            ('dropout5', nn.Dropout(0.3)),\n",
    "            ('fc3', nn.Linear(512, 256)),\n",
    "            ('bn_fc3', nn.BatchNorm1d(256)),\n",
    "            ('relu_fc3', nn.ReLU(inplace=True)),\n",
    "            ('dropout6', nn.Dropout(0.2)),\n",
    "            ('fc4', nn.Linear(256, 10)),\n",
    "            ('log_softmax', nn.LogSoftmax(dim=1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Créer le modèle optimisé\n",
    "model = EnhancedMNISTNet()\n",
    "print(\"Architecture du modèle OPTIMISÉ avec Sequential:\")\n",
    "print(model)\n",
    "\n",
    "# Calculer le nombre de paramètres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParamètres totaux: {total_params:,}\")\n",
    "print(f\"Paramètres entraînables: {trainable_params:,}\")\n",
    "\n",
    "# Ajouter le modèle au TensorBoard\n",
    "writer = SummaryWriter(log_dir)\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "writer.add_graph(model, dummy_input)\n",
    "print(\"Graphe du modèle optimisé ajouté à TensorBoard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7741e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entraînement: 60000 échantillons\n",
      "Données de test: 10000 échantillons\n",
      "AUGMENTATION DE DONNÉES ACTIVÉE pour un surentraînement optimal!\n",
      "Échantillons et statistiques d'entraînement renforcé ajoutés à TensorBoard\n",
      "Échantillons et statistiques d'entraînement renforcé ajoutés à TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Transformations d'augmentation pour l'entraînement\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "# Suppression du multiprocessing pour éviter les erreurs de pickle\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Données d'entraînement: {len(train_dataset)} échantillons\")\n",
    "print(f\"Données de test: {len(test_dataset)} échantillons\")\n",
    "print(\"AUGMENTATION DE DONNÉES ACTIVÉE pour un surentraînement optimal!\")\n",
    "\n",
    "# Ajouter des échantillons à TensorBoard\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Créer une grille d'images pour TensorBoard\n",
    "img_grid = torchvision.utils.make_grid(images[:32])\n",
    "writer.add_image('MNIST_Images_Sample_Enhanced', img_grid)\n",
    "\n",
    "# Ajouter les embeddings des données\n",
    "writer.add_embedding(images.view(-1, 28*28)[:100], \n",
    "                    metadata=labels[:100].tolist(),\n",
    "                    tag='MNIST_Embeddings_Enhanced')\n",
    "\n",
    "# Distribution des classes\n",
    "labels_list = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "class_counts = np.bincount(labels_list)\n",
    "\n",
    "# Ajouter l'histogramme des classes à TensorBoard\n",
    "for i, count in enumerate(class_counts):\n",
    "    writer.add_scalar(f'Dataset/Class_{i}_Count', count, 0)\n",
    "\n",
    "print(\"Échantillons et statistiques d'entraînement renforcé ajoutés à TensorBoard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db72ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4: Fonctions d'entraînement et de test RENFORCÉES\n",
    "class EnhancedTensorBoardTracker:\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.best_accuracy = 0\n",
    "        \n",
    "    def update(self, epoch, train_loss, test_loss, test_acc, lr, model=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        # Suivre la meilleure accuracy\n",
    "        if test_acc > self.best_accuracy:\n",
    "            self.best_accuracy = test_acc\n",
    "            print(f\"NOUVELLE MEILLEURE ACCURACY: {test_acc:.3f}%\")\n",
    "        \n",
    "        # Enregistrer les métriques dans TensorBoard\n",
    "        self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        self.writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "        self.writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "        self.writer.add_scalar('Accuracy/Best', self.best_accuracy, epoch)\n",
    "        self.writer.add_scalar('Learning_Rate', lr, epoch)\n",
    "        \n",
    "        # Ajouter les histogrammes des poids et gradients\n",
    "        if model is not None:\n",
    "            for name, param in model.named_parameters():\n",
    "                self.writer.add_histogram(f'Weights/{name}', param, epoch)\n",
    "                if param.grad is not None:\n",
    "                    self.writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "                    # Surveillance des gradients\n",
    "                    grad_norm = param.grad.norm().item()\n",
    "                    self.writer.add_scalar(f'Gradient_Norms/{name}', grad_norm, epoch)\n",
    "\n",
    "def train_epoch_enhanced(model, device, train_loader, optimizer, epoch, tracker=None, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "        \n",
    "        if batch_idx % 100 == 0:  # Affichage toutes les 100 itérations\n",
    "            current_acc = 100. * correct / processed\n",
    "            print(f'Époque {epoch}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.6f}, Accuracy: {current_acc:.2f}%')\n",
    "            \n",
    "            # Ajouter la loss par batch à TensorBoard\n",
    "            global_step = (epoch - 1) * len(train_loader) + batch_idx\n",
    "            if tracker and tracker.writer:\n",
    "                tracker.writer.add_scalar('Loss/Train_Batch', loss.item(), global_step)\n",
    "                tracker.writer.add_scalar('Accuracy/Train_Batch', current_acc, global_step)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_model_enhanced(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            # Calculer les confidences\n",
    "            probs = torch.exp(output)\n",
    "            confidences = probs.max(dim=1)[0]\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    avg_confidence = np.mean(all_confidences)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.3f}%, Confiance moyenne: {avg_confidence:.3f}')\n",
    "    \n",
    "    return test_loss, accuracy, all_preds, all_targets, avg_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22e22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5: Entraînement optimisé avec TensorBoard\n",
    "def train_model_with_tensorboard():\n",
    "    print(\"DÉBUT DE L'ENTRAÎNEMENT OPTIMISÉ AVEC TENSORBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialiser le modèle renforcé\n",
    "    model = EnhancedMNISTNet().to(device)\n",
    "    \n",
    "    # Optimiseur avec paramètres équilibrés\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "    \n",
    "    # Scheduler adapté pour 10 époques\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.005, epochs=10, steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3, anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    tracker = EnhancedTensorBoardTracker(writer)\n",
    "    \n",
    "    # Entraînement optimisé: 10 époques\n",
    "    num_epochs = 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Entraînement configuré pour {num_epochs} époques avec {len(train_loader)} batchs par époque\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ÉPOQUE {epoch}/{num_epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Entraînement avec la fonction améliorée\n",
    "        train_loss, train_acc = train_epoch_enhanced(\n",
    "            model, device, train_loader, optimizer, epoch, tracker, scheduler\n",
    "        )\n",
    "        \n",
    "        # Test avec métriques étendues\n",
    "        test_loss, test_acc, preds, targets, avg_conf = test_model_enhanced(\n",
    "            model, device, test_loader\n",
    "        )\n",
    "        \n",
    "        # Update du scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Tracking avec TensorBoard\n",
    "        tracker.update(epoch, train_loss, test_loss, test_acc, current_lr, model)\n",
    "        \n",
    "        # Ajouter métriques additionnelles\n",
    "        writer.add_scalar('Confidence/Average', avg_conf, epoch)\n",
    "        writer.add_scalar('Training/Train_Accuracy', train_acc, epoch)\n",
    "        \n",
    "        # Matrices de confusion toutes les 3 époques\n",
    "        if epoch % 3 == 0 or epoch == 1:\n",
    "            cm = confusion_matrix(targets, preds)\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar_kws={'shrink': 0.8})\n",
    "            ax.set_title(f'Matrice de Confusion - Époque {epoch}\\nAccuracy: {test_acc:.3f}%')\n",
    "            ax.set_ylabel('Vraie classe')\n",
    "            ax.set_xlabel('Classe prédite')\n",
    "            writer.add_figure(f'Confusion_Matrix/Epoch_{epoch}', fig, epoch)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        print(f\"Époque {epoch} terminée - Train Acc: {train_acc:.3f}%, Test Acc: {test_acc:.3f}%\")\n",
    "        print(f\"Confiance moyenne: {avg_conf:.3f}, LR: {current_lr:.6f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ENTRAÎNEMENT TERMINÉ en {total_time:.2f} secondes!\")\n",
    "    print(f\"Accuracy finale: {tracker.test_accuracies[-1]:.3f}%\")\n",
    "    print(f\"MEILLEURE ACCURACY ATTEINTE: {tracker.best_accuracy:.3f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Rapport de classification final\n",
    "    print(\"\\nRapport de classification FINAL:\")\n",
    "    report = classification_report(targets, preds, target_names=[str(i) for i in range(10)], digits=3)\n",
    "    print(report)\n",
    "    \n",
    "    # Ajouter le rapport de classification à TensorBoard\n",
    "    class_report = classification_report(targets, preds, target_names=[str(i) for i in range(10)], output_dict=True)\n",
    "    for class_name, metrics in class_report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            for metric_name, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    writer.add_scalar(f'Final_Classification_Report/{class_name}_{metric_name}', value, num_epochs)\n",
    "    \n",
    "    return model, tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d23ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7: Export ONNX avec validation\n",
    "def export_and_validate_onnx(model, device, model_path='mnist_model.onnx'):\n",
    "    print(\"Export du modèle en ONNX...\")\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "    \n",
    "    # Export ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        model_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Modèle exporté vers {model_path}\")\n",
    "    \n",
    "    # Validation ONNX\n",
    "    onnx_model = onnx.load(model_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"Modèle ONNX validé!\")\n",
    "    \n",
    "    # Test de comparaison PyTorch vs ONNX\n",
    "    print(\"Comparaison PyTorch vs ONNX...\")\n",
    "    \n",
    "    ort_session = ort.InferenceSession(model_path)\n",
    "    \n",
    "    # Test sur quelques échantillons\n",
    "    test_data, test_targets = next(iter(test_loader))\n",
    "    test_sample = test_data[:5].to(device)\n",
    "    \n",
    "    # Prédiction PyTorch\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = model(test_sample)\n",
    "        pytorch_pred = pytorch_output.argmax(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Prédiction ONNX\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: test_sample.cpu().numpy()}\n",
    "    onnx_output = ort_session.run(None, ort_inputs)[0]\n",
    "    onnx_pred = np.argmax(onnx_output, axis=1)\n",
    "    \n",
    "    # Comparaison\n",
    "    print(\"Comparaison des prédictions:\")\n",
    "    matches = 0\n",
    "    for i in range(5):\n",
    "        match = pytorch_pred[i] == onnx_pred[i]\n",
    "        matches += match\n",
    "        print(f\"Échantillon {i+1}: PyTorch={pytorch_pred[i]}, ONNX={onnx_pred[i]}, \"\n",
    "              f\"Match={'OUI' if match else 'NON'}\")\n",
    "    \n",
    "    # Différence dans les probabilités\n",
    "    pytorch_probs = torch.exp(pytorch_output).cpu().numpy()\n",
    "    onnx_probs = np.exp(onnx_output)  # Convertir log_softmax en probabilités\n",
    "    \n",
    "    max_diff = np.max(np.abs(pytorch_probs - onnx_probs))\n",
    "    print(f\"Différence maximale entre les probabilités: {max_diff:.6f}\")\n",
    "    \n",
    "    if max_diff < 1e-5:\n",
    "        print(\"Les modèles PyTorch et ONNX produisent des résultats identiques!\")\n",
    "    else:\n",
    "        print(\"Petites différences détectées (normal due à la précision)\")\n",
    "    \n",
    "    # Ajouter les résultats de comparaison à TensorBoard\n",
    "    writer.add_scalar('ONNX_Validation/Prediction_Matches', matches, 0)\n",
    "    writer.add_scalar('ONNX_Validation/Max_Probability_Difference', max_diff, 0)\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14762d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DÉMARRAGE DU PIPELINE COMPLET MNIST\n",
      "============================================================\n",
      "DÉBUT DE L'ENTRAÎNEMENT OPTIMISÉ AVEC TENSORBOARD\n",
      "============================================================\n",
      "Entraînement configuré pour 10 époques avec 469 batchs par époque\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 1/10\n",
      "==================================================\n",
      "Époque 1, Batch 0/469, Loss: 2.397455, Accuracy: 12.50%\n",
      "Époque 1, Batch 0/469, Loss: 2.397455, Accuracy: 12.50%\n",
      "Époque 1, Batch 100/469, Loss: 0.271298, Accuracy: 77.48%\n",
      "Époque 1, Batch 100/469, Loss: 0.271298, Accuracy: 77.48%\n",
      "Époque 1, Batch 200/469, Loss: 0.094768, Accuracy: 87.18%\n",
      "Époque 1, Batch 200/469, Loss: 0.094768, Accuracy: 87.18%\n",
      "Époque 1, Batch 300/469, Loss: 0.048194, Accuracy: 90.68%\n",
      "Époque 1, Batch 300/469, Loss: 0.048194, Accuracy: 90.68%\n",
      "Époque 1, Batch 400/469, Loss: 0.089169, Accuracy: 92.51%\n",
      "Époque 1, Batch 400/469, Loss: 0.089169, Accuracy: 92.51%\n",
      "Test Loss: 0.0299, Accuracy: 99.130%, Confiance moyenne: 0.988\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.130%\n",
      "Test Loss: 0.0299, Accuracy: 99.130%, Confiance moyenne: 0.988\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.130%\n",
      "Époque 1 terminée - Train Acc: 93.335%, Test Acc: 99.130%\n",
      "Confiance moyenne: 0.988, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 2/10\n",
      "==================================================\n",
      "Époque 2, Batch 0/469, Loss: 0.086912, Accuracy: 97.66%\n",
      "Époque 1 terminée - Train Acc: 93.335%, Test Acc: 99.130%\n",
      "Confiance moyenne: 0.988, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 2/10\n",
      "==================================================\n",
      "Époque 2, Batch 0/469, Loss: 0.086912, Accuracy: 97.66%\n",
      "Époque 2, Batch 100/469, Loss: 0.020830, Accuracy: 98.81%\n",
      "Époque 2, Batch 100/469, Loss: 0.020830, Accuracy: 98.81%\n",
      "Époque 2, Batch 200/469, Loss: 0.123944, Accuracy: 98.67%\n",
      "Époque 2, Batch 200/469, Loss: 0.123944, Accuracy: 98.67%\n",
      "Époque 2, Batch 300/469, Loss: 0.077873, Accuracy: 98.68%\n",
      "Époque 2, Batch 300/469, Loss: 0.077873, Accuracy: 98.68%\n",
      "Époque 2, Batch 400/469, Loss: 0.065832, Accuracy: 98.69%\n",
      "Époque 2, Batch 400/469, Loss: 0.065832, Accuracy: 98.69%\n",
      "Test Loss: 0.0228, Accuracy: 99.220%, Confiance moyenne: 0.993\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.220%\n",
      "Époque 2 terminée - Train Acc: 98.703%, Test Acc: 99.220%\n",
      "Confiance moyenne: 0.993, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 3/10\n",
      "==================================================\n",
      "Test Loss: 0.0228, Accuracy: 99.220%, Confiance moyenne: 0.993\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.220%\n",
      "Époque 2 terminée - Train Acc: 98.703%, Test Acc: 99.220%\n",
      "Confiance moyenne: 0.993, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 3/10\n",
      "==================================================\n",
      "Époque 3, Batch 0/469, Loss: 0.017216, Accuracy: 99.22%\n",
      "Époque 3, Batch 0/469, Loss: 0.017216, Accuracy: 99.22%\n",
      "Époque 3, Batch 100/469, Loss: 0.026058, Accuracy: 99.03%\n",
      "Époque 3, Batch 100/469, Loss: 0.026058, Accuracy: 99.03%\n",
      "Époque 3, Batch 200/469, Loss: 0.014848, Accuracy: 98.94%\n",
      "Époque 3, Batch 200/469, Loss: 0.014848, Accuracy: 98.94%\n",
      "Époque 3, Batch 300/469, Loss: 0.022655, Accuracy: 98.94%\n",
      "Époque 3, Batch 300/469, Loss: 0.022655, Accuracy: 98.94%\n",
      "Époque 3, Batch 400/469, Loss: 0.017083, Accuracy: 98.95%\n",
      "Époque 3, Batch 400/469, Loss: 0.017083, Accuracy: 98.95%\n",
      "Test Loss: 0.0199, Accuracy: 99.410%, Confiance moyenne: 0.995\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.410%\n",
      "Test Loss: 0.0199, Accuracy: 99.410%, Confiance moyenne: 0.995\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.410%\n",
      "Époque 3 terminée - Train Acc: 98.927%, Test Acc: 99.410%\n",
      "Confiance moyenne: 0.995, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 4/10\n",
      "==================================================\n",
      "Époque 4, Batch 0/469, Loss: 0.036790, Accuracy: 98.44%\n",
      "Époque 3 terminée - Train Acc: 98.927%, Test Acc: 99.410%\n",
      "Confiance moyenne: 0.995, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 4/10\n",
      "==================================================\n",
      "Époque 4, Batch 0/469, Loss: 0.036790, Accuracy: 98.44%\n",
      "Époque 4, Batch 100/469, Loss: 0.028819, Accuracy: 99.03%\n",
      "Époque 4, Batch 100/469, Loss: 0.028819, Accuracy: 99.03%\n",
      "Époque 4, Batch 200/469, Loss: 0.024038, Accuracy: 99.11%\n",
      "Époque 4, Batch 200/469, Loss: 0.024038, Accuracy: 99.11%\n",
      "Époque 4, Batch 300/469, Loss: 0.010384, Accuracy: 99.07%\n",
      "Époque 4, Batch 300/469, Loss: 0.010384, Accuracy: 99.07%\n",
      "Époque 4, Batch 400/469, Loss: 0.010513, Accuracy: 99.08%\n",
      "Époque 4, Batch 400/469, Loss: 0.010513, Accuracy: 99.08%\n",
      "Test Loss: 0.0179, Accuracy: 99.510%, Confiance moyenne: 0.996\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.510%\n",
      "Époque 4 terminée - Train Acc: 99.063%, Test Acc: 99.510%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 5/10\n",
      "==================================================\n",
      "Test Loss: 0.0179, Accuracy: 99.510%, Confiance moyenne: 0.996\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.510%\n",
      "Époque 4 terminée - Train Acc: 99.063%, Test Acc: 99.510%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 5/10\n",
      "==================================================\n",
      "Époque 5, Batch 0/469, Loss: 0.018438, Accuracy: 100.00%\n",
      "Époque 5, Batch 0/469, Loss: 0.018438, Accuracy: 100.00%\n",
      "Époque 5, Batch 100/469, Loss: 0.003429, Accuracy: 99.28%\n",
      "Époque 5, Batch 100/469, Loss: 0.003429, Accuracy: 99.28%\n",
      "Époque 5, Batch 200/469, Loss: 0.011320, Accuracy: 99.25%\n",
      "Époque 5, Batch 200/469, Loss: 0.011320, Accuracy: 99.25%\n",
      "Époque 5, Batch 300/469, Loss: 0.048735, Accuracy: 99.21%\n",
      "Époque 5, Batch 300/469, Loss: 0.048735, Accuracy: 99.21%\n",
      "Époque 5, Batch 400/469, Loss: 0.011350, Accuracy: 99.22%\n",
      "Époque 5, Batch 400/469, Loss: 0.011350, Accuracy: 99.22%\n",
      "Test Loss: 0.0186, Accuracy: 99.440%, Confiance moyenne: 0.996\n",
      "Époque 5 terminée - Train Acc: 99.215%, Test Acc: 99.440%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 6/10\n",
      "==================================================\n",
      "Test Loss: 0.0186, Accuracy: 99.440%, Confiance moyenne: 0.996\n",
      "Époque 5 terminée - Train Acc: 99.215%, Test Acc: 99.440%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 6/10\n",
      "==================================================\n",
      "Époque 6, Batch 0/469, Loss: 0.005050, Accuracy: 100.00%\n",
      "Époque 6, Batch 0/469, Loss: 0.005050, Accuracy: 100.00%\n",
      "Époque 6, Batch 100/469, Loss: 0.061773, Accuracy: 99.22%\n",
      "Époque 6, Batch 100/469, Loss: 0.061773, Accuracy: 99.22%\n",
      "Époque 6, Batch 200/469, Loss: 0.009448, Accuracy: 99.32%\n",
      "Époque 6, Batch 200/469, Loss: 0.009448, Accuracy: 99.32%\n",
      "Époque 6, Batch 300/469, Loss: 0.010019, Accuracy: 99.34%\n",
      "Époque 6, Batch 300/469, Loss: 0.010019, Accuracy: 99.34%\n",
      "Époque 6, Batch 400/469, Loss: 0.018948, Accuracy: 99.36%\n",
      "Époque 6, Batch 400/469, Loss: 0.018948, Accuracy: 99.36%\n",
      "Test Loss: 0.0165, Accuracy: 99.480%, Confiance moyenne: 0.996\n",
      "Test Loss: 0.0165, Accuracy: 99.480%, Confiance moyenne: 0.996\n",
      "Époque 6 terminée - Train Acc: 99.358%, Test Acc: 99.480%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 7/10\n",
      "==================================================\n",
      "Époque 7, Batch 0/469, Loss: 0.002649, Accuracy: 100.00%\n",
      "Époque 6 terminée - Train Acc: 99.358%, Test Acc: 99.480%\n",
      "Confiance moyenne: 0.996, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 7/10\n",
      "==================================================\n",
      "Époque 7, Batch 0/469, Loss: 0.002649, Accuracy: 100.00%\n",
      "Époque 7, Batch 100/469, Loss: 0.115848, Accuracy: 99.54%\n",
      "Époque 7, Batch 100/469, Loss: 0.115848, Accuracy: 99.54%\n",
      "Époque 7, Batch 200/469, Loss: 0.002263, Accuracy: 99.50%\n",
      "Époque 7, Batch 200/469, Loss: 0.002263, Accuracy: 99.50%\n",
      "Époque 7, Batch 300/469, Loss: 0.049182, Accuracy: 99.49%\n",
      "Époque 7, Batch 300/469, Loss: 0.049182, Accuracy: 99.49%\n",
      "Époque 7, Batch 400/469, Loss: 0.004043, Accuracy: 99.45%\n",
      "Époque 7, Batch 400/469, Loss: 0.004043, Accuracy: 99.45%\n",
      "Test Loss: 0.0151, Accuracy: 99.530%, Confiance moyenne: 0.997\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.530%\n",
      "Époque 7 terminée - Train Acc: 99.417%, Test Acc: 99.530%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 8/10\n",
      "==================================================\n",
      "Test Loss: 0.0151, Accuracy: 99.530%, Confiance moyenne: 0.997\n",
      "NOUVELLE MEILLEURE ACCURACY: 99.530%\n",
      "Époque 7 terminée - Train Acc: 99.417%, Test Acc: 99.530%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 8/10\n",
      "==================================================\n",
      "Époque 8, Batch 0/469, Loss: 0.004095, Accuracy: 100.00%\n",
      "Époque 8, Batch 0/469, Loss: 0.004095, Accuracy: 100.00%\n",
      "Époque 8, Batch 100/469, Loss: 0.001202, Accuracy: 99.44%\n",
      "Époque 8, Batch 100/469, Loss: 0.001202, Accuracy: 99.44%\n",
      "Époque 8, Batch 200/469, Loss: 0.006075, Accuracy: 99.48%\n",
      "Époque 8, Batch 200/469, Loss: 0.006075, Accuracy: 99.48%\n",
      "Époque 8, Batch 300/469, Loss: 0.002752, Accuracy: 99.48%\n",
      "Époque 8, Batch 300/469, Loss: 0.002752, Accuracy: 99.48%\n",
      "Époque 8, Batch 400/469, Loss: 0.014908, Accuracy: 99.49%\n",
      "Époque 8, Batch 400/469, Loss: 0.014908, Accuracy: 99.49%\n",
      "Test Loss: 0.0163, Accuracy: 99.470%, Confiance moyenne: 0.997\n",
      "Époque 8 terminée - Train Acc: 99.498%, Test Acc: 99.470%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 9/10\n",
      "==================================================\n",
      "Test Loss: 0.0163, Accuracy: 99.470%, Confiance moyenne: 0.997\n",
      "Époque 8 terminée - Train Acc: 99.498%, Test Acc: 99.470%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 9/10\n",
      "==================================================\n",
      "Époque 9, Batch 0/469, Loss: 0.001453, Accuracy: 100.00%\n",
      "Époque 9, Batch 0/469, Loss: 0.001453, Accuracy: 100.00%\n",
      "Époque 9, Batch 100/469, Loss: 0.022935, Accuracy: 99.57%\n",
      "Époque 9, Batch 100/469, Loss: 0.022935, Accuracy: 99.57%\n",
      "Époque 9, Batch 200/469, Loss: 0.012893, Accuracy: 99.53%\n",
      "Époque 9, Batch 200/469, Loss: 0.012893, Accuracy: 99.53%\n",
      "Époque 9, Batch 300/469, Loss: 0.001185, Accuracy: 99.53%\n",
      "Époque 9, Batch 300/469, Loss: 0.001185, Accuracy: 99.53%\n",
      "Époque 9, Batch 400/469, Loss: 0.002045, Accuracy: 99.51%\n",
      "Époque 9, Batch 400/469, Loss: 0.002045, Accuracy: 99.51%\n",
      "Test Loss: 0.0198, Accuracy: 99.420%, Confiance moyenne: 0.997\n",
      "Test Loss: 0.0198, Accuracy: 99.420%, Confiance moyenne: 0.997\n",
      "Époque 9 terminée - Train Acc: 99.520%, Test Acc: 99.420%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 10/10\n",
      "==================================================\n",
      "Époque 10, Batch 0/469, Loss: 0.000759, Accuracy: 100.00%\n",
      "Époque 9 terminée - Train Acc: 99.520%, Test Acc: 99.420%\n",
      "Confiance moyenne: 0.997, LR: 0.000200\n",
      "\n",
      "==================================================\n",
      "ÉPOQUE 10/10\n",
      "==================================================\n",
      "Époque 10, Batch 0/469, Loss: 0.000759, Accuracy: 100.00%\n",
      "Époque 10, Batch 100/469, Loss: 0.003831, Accuracy: 99.54%\n",
      "Époque 10, Batch 100/469, Loss: 0.003831, Accuracy: 99.54%\n",
      "Époque 10, Batch 200/469, Loss: 0.006867, Accuracy: 99.48%\n",
      "Époque 10, Batch 200/469, Loss: 0.006867, Accuracy: 99.48%\n",
      "Époque 10, Batch 300/469, Loss: 0.009774, Accuracy: 99.51%\n",
      "Époque 10, Batch 300/469, Loss: 0.009774, Accuracy: 99.51%\n",
      "Époque 10, Batch 400/469, Loss: 0.001777, Accuracy: 99.52%\n",
      "Époque 10, Batch 400/469, Loss: 0.001777, Accuracy: 99.52%\n",
      "Test Loss: 0.0219, Accuracy: 99.350%, Confiance moyenne: 0.996\n",
      "Époque 10 terminée - Train Acc: 99.503%, Test Acc: 99.350%\n",
      "Confiance moyenne: 0.996, LR: 0.000201\n",
      "\n",
      "==================================================\n",
      "ENTRAÎNEMENT TERMINÉ en 299.80 secondes!\n",
      "Accuracy finale: 99.350%\n",
      "MEILLEURE ACCURACY ATTEINTE: 99.530%\n",
      "==================================================\n",
      "\n",
      "Rapport de classification FINAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.992     0.993       980\n",
      "           1      0.997     0.996     0.997      1135\n",
      "           2      0.997     0.995     0.996      1032\n",
      "           3      0.996     0.995     0.996      1010\n",
      "           4      0.996     0.985     0.990       982\n",
      "           5      0.989     0.996     0.992       892\n",
      "           6      0.991     0.991     0.991       958\n",
      "           7      0.993     0.996     0.995      1028\n",
      "           8      0.996     0.997     0.996       974\n",
      "           9      0.984     0.992     0.988      1009\n",
      "\n",
      "    accuracy                          0.994     10000\n",
      "   macro avg      0.993     0.993     0.993     10000\n",
      "weighted avg      0.994     0.994     0.994     10000\n",
      "\n",
      "Test Loss: 0.0219, Accuracy: 99.350%, Confiance moyenne: 0.996\n",
      "Époque 10 terminée - Train Acc: 99.503%, Test Acc: 99.350%\n",
      "Confiance moyenne: 0.996, LR: 0.000201\n",
      "\n",
      "==================================================\n",
      "ENTRAÎNEMENT TERMINÉ en 299.80 secondes!\n",
      "Accuracy finale: 99.350%\n",
      "MEILLEURE ACCURACY ATTEINTE: 99.530%\n",
      "==================================================\n",
      "\n",
      "Rapport de classification FINAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.992     0.993       980\n",
      "           1      0.997     0.996     0.997      1135\n",
      "           2      0.997     0.995     0.996      1032\n",
      "           3      0.996     0.995     0.996      1010\n",
      "           4      0.996     0.985     0.990       982\n",
      "           5      0.989     0.996     0.992       892\n",
      "           6      0.991     0.991     0.991       958\n",
      "           7      0.993     0.996     0.995      1028\n",
      "           8      0.996     0.997     0.996       974\n",
      "           9      0.984     0.992     0.988      1009\n",
      "\n",
      "    accuracy                          0.994     10000\n",
      "   macro avg      0.993     0.993     0.993     10000\n",
      "weighted avg      0.994     0.994     0.994     10000\n",
      "\n",
      "ANALYSE DES PRÉDICTIONS\n",
      "========================================\n",
      "EXPORT ONNX\n",
      "========================================\n",
      "Export du modèle en ONNX...\n",
      "ANALYSE DES PRÉDICTIONS\n",
      "========================================\n",
      "EXPORT ONNX\n",
      "========================================\n",
      "Export du modèle en ONNX...\n",
      "Modèle exporté vers mnist_model.onnx\n",
      "Modèle ONNX validé!\n",
      "Comparaison PyTorch vs ONNX...\n",
      "Modèle exporté vers mnist_model.onnx\n",
      "Modèle ONNX validé!\n",
      "Comparaison PyTorch vs ONNX...\n",
      "Comparaison des prédictions:\n",
      "Échantillon 1: PyTorch=7, ONNX=7, Match=OUI\n",
      "Échantillon 2: PyTorch=2, ONNX=2, Match=OUI\n",
      "Échantillon 3: PyTorch=1, ONNX=1, Match=OUI\n",
      "Échantillon 4: PyTorch=0, ONNX=0, Match=OUI\n",
      "Échantillon 5: PyTorch=4, ONNX=4, Match=OUI\n",
      "Différence maximale entre les probabilités: 0.000000\n",
      "Les modèles PyTorch et ONNX produisent des résultats identiques!\n",
      "Modèle PyTorch sauvegardé: mnist_model.pth\n",
      "INSTRUCTIONS DE DÉPLOIEMENT\n",
      "========================================\n",
      "PIPELINE TERMINÉ AVEC SUCCÈS!\n",
      "Accuracy finale: 99.35%\n",
      "Fichiers générés:\n",
      "   - mnist_model.pth (modèle PyTorch)\n",
      "   - mnist_model.onnx (modèle ONNX pour le web)\n",
      "Comparaison des prédictions:\n",
      "Échantillon 1: PyTorch=7, ONNX=7, Match=OUI\n",
      "Échantillon 2: PyTorch=2, ONNX=2, Match=OUI\n",
      "Échantillon 3: PyTorch=1, ONNX=1, Match=OUI\n",
      "Échantillon 4: PyTorch=0, ONNX=0, Match=OUI\n",
      "Échantillon 5: PyTorch=4, ONNX=4, Match=OUI\n",
      "Différence maximale entre les probabilités: 0.000000\n",
      "Les modèles PyTorch et ONNX produisent des résultats identiques!\n",
      "Modèle PyTorch sauvegardé: mnist_model.pth\n",
      "INSTRUCTIONS DE DÉPLOIEMENT\n",
      "========================================\n",
      "PIPELINE TERMINÉ AVEC SUCCÈS!\n",
      "Accuracy finale: 99.35%\n",
      "Fichiers générés:\n",
      "   - mnist_model.pth (modèle PyTorch)\n",
      "   - mnist_model.onnx (modèle ONNX pour le web)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cellule 9: Fonction principale pour exécuter tout le pipeline\n",
    "\n",
    "print(\"DÉMARRAGE DU PIPELINE COMPLET MNIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Entraînement OPTIMISÉ\n",
    "model, tracker = train_model_with_tensorboard()\n",
    "\n",
    "# 2. Analyse des prédictions\n",
    "print(\"ANALYSE DES PRÉDICTIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 3. Export ONNX\n",
    "print(\"EXPORT ONNX\")\n",
    "print(\"=\"*40)\n",
    "onnx_path = export_and_validate_onnx(model, device)\n",
    "\n",
    "# 4. Sauvegarde PyTorch\n",
    "torch.save(model.state_dict(), 'mnist_model.pth')\n",
    "print(\"Modèle PyTorch sauvegardé: mnist_model.pth\")\n",
    "\n",
    "# 5. Instructions de déploiement\n",
    "print(\"INSTRUCTIONS DE DÉPLOIEMENT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"PIPELINE TERMINÉ AVEC SUCCÈS!\")\n",
    "print(f\"Accuracy finale: {tracker.test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Fichiers générés:\")\n",
    "print(f\"   - mnist_model.pth (modèle PyTorch)\")\n",
    "print(f\"   - {onnx_path} (modèle ONNX pour le web)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
